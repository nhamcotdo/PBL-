{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12932,"status":"ok","timestamp":1679564416403,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"T1AgodtqgQPC","outputId":"0212405e-d102-4af2-a8d0-e0bd4d7af4e9"},"outputs":[],"source":["# !pip install mediapipe"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25895,"status":"ok","timestamp":1679564442291,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"r5J-cqN0gTa3","outputId":"9235516d-8661-41ec-b348-4fb0e300f26a"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3boTmYvYgOe8"},"outputs":[],"source":["import cv2\n","import mediapipe as mp\n","import numpy as np\n","import os\n","import pandas as pd\n","from PIL import Image\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"13D9dXX5gOfA"},"outputs":[],"source":["# Kích thướng mỗi frame\n","IMAGE_WIDTH,  IMAGE_HEIGHT = 160, 120\n","# số lượng frame sẽ lấy ở mỗi video\n","SEQUENCE_LENGTH = 20\n","# Những hành động sẽ nhận dạng\n","CLASSES_LIST = [\n","    'Your', 'Love', 'Sleep', 'Name',\n","    'What', 'Read', 'I', 'Bye', 'You', 'Eat',\n","    'Hello']\n","\n","# Link thư mục chứa video train\n","# DATASET_DIR = './drive/MyDrive/PBL5/DaPhanLoai/'\n","DATASET_DIR = './DaPhanLoai'\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1679565201293,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"t3uxp2zugOfB"},"outputs":[],"source":["mp_drawing = mp.solutions.drawing_utils\n","mp_drawing_styles = mp.solutions.drawing_styles\n","mp_pose = mp.solutions.pose\n","mp_hands = mp.solutions.hands\n","\n","\n","def frames_extraction(video_path):\n","    '''\n","    This function will extract the required frames from a video after resizing and normalizing them.\n","    Args:\n","        video_path: The path of the video in the disk, whose frames are to be extracted.\n","    Returns:\n","        frames_list: A list containing the resized and normalized frames of the video.\n","    '''\n","\n","    video_reader = cv2.VideoCapture(video_path)\n","    video_frames_count = 0\n","    s, f = video_reader.read()\n","    while s:\n","        s, f = video_reader.read()\n","        video_frames_count += 1\n","    video_reader.release()\n","\n","    # ĐỌc video\n","\n","    # Calculate the the interval after which frames will be added to the list.\n","    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n","\n","    # Danh sách chứa các frame sẽ lấy\n","    frames_list = []\n","    origin_frames_list = []\n","\n","    # Iterate through the Video Frames.\n","    with mp_hands.Hands(\n","            static_image_mode=True,\n","            max_num_hands=2,\n","            min_detection_confidence=0.5) as hands:\n","        with mp_pose.Pose(\n","                min_detection_confidence=0.5,\n","                min_tracking_confidence=0.5) as pose:\n","\n","            cap = cv2.VideoCapture(video_path)\n","            for frame_counter in range(SEQUENCE_LENGTH):\n","                cap.set(cv2.CAP_PROP_POS_FRAMES,\n","                        frame_counter * skip_frames_window)\n","                success, image = cap.read()\n","\n","                if not success:\n","                    cap = cv2.VideoCapture(video_path)\n","                    continue\n","                    # break\n","\n","                # resize image to half its original size\n","                image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n","\n","                # To improve performance, optionally mark the image as not writeable to\n","                image.flags.writeable = True\n","                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","                hand_results = hands.process(image)\n","                pose_results = pose.process(image)\n","\n","                image2 = np.zeros(\n","                    (image.shape[0], image.shape[1], 3), np.uint8)\n","\n","                # # Draw the hand annotation on the image.\n","                if hand_results.multi_hand_landmarks:\n","                    for hand_landmarks in hand_results.multi_hand_landmarks:\n","                        mp_drawing.draw_landmarks(\n","                            image2,\n","                            hand_landmarks,\n","                            mp_hands.HAND_CONNECTIONS,\n","                            mp_drawing_styles.get_default_hand_landmarks_style(),\n","                            mp_drawing_styles.get_default_hand_connections_style())\n","\n","                # Draw the pose annotation on the image.\n","                image.flags.writeable = False\n","                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","                mp_drawing.draw_landmarks(\n","                    image2,\n","                    pose_results.pose_landmarks,\n","                    mp_pose.POSE_CONNECTIONS,\n","                    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n","\n","                # Append to list feature\n","\n","                image.flags.writeable = True\n","                \n","                frames_list.append(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)/255)\n","                origin_frames_list.append(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)/255)\n","\n","                # resized_frame = cv2.resize(image2, (IMAGE_WIDTH, IMAGE_HEIGHT))\n","                # frames_list.append(np.any(resized_frame != 0, axis=-1).astype(int))\n","\n","                # Flip the image2 horizontally for a selfie-view display.\n","                cv2.imshow('MediaPipe Pose', cv2.flip(image2, 1))\n","                if cv2.waitKey(1) == ord('e'):\n","                    return\n","\n","    # Release the VideoCapture object.\n","    # Return the frames list.\n","    return frames_list, origin_frames_list\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"lI6nxlQNgOfE"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/nhamcotdo/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"]}],"source":["frames = frames_extraction(f'{DATASET_DIR}/Bye/076.avi')\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1679564450463,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"4orsuTkNylk-","outputId":"aee3551b-4c85-4d5b-c61a-ba5ec70fe7cf"},"outputs":[{"data":{"text/plain":["(20, 120, 160)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["np.shape(frames[1])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Jn2Yclm-gOfG"},"outputs":[],"source":["def create_dataset():\n","    features = []\n","    labels = []\n","    origin_features = []\n","\n","    video_files_paths = []\n","    for class_index, class_name in enumerate(CLASSES_LIST):\n","        feature = []\n","        origin_feature = []\n","        label = []\n","        video_files_path = []\n","\n","        # Display the name of the class whose data is being extracted.\n","        print(f'Extracting Data of Class: {class_name}')\n","\n","        # Get the list of video files present in the specific class name directory.\n","        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n","\n","        # Iterate through all the files present in the files list.\n","        for file_name in files_list:\n","            # print(file_name)\n","            try:\n","                # Get the complete video path.\n","                video_file_path = os.path.join(\n","                    DATASET_DIR, class_name, file_name)\n","\n","                # Extract the frames of the video file.\n","                frames = frames_extraction(video_file_path)\n","\n","                # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n","                # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n","                if len(frames[0]) == SEQUENCE_LENGTH:\n","\n","                    # Append the data to their repective lists.\n","                    # feature.append(frames[0])\n","                    # origin_feature.append(frames[1])\n","                    features.append(frames[0])\n","                    origin_features.append(frames[1])\n","                    # label.append(class_index)\n","                    labels.append(class_index)\n","                    # video_files_path.append(video_file_path)\n","                    video_files_paths.append(video_file_path)\n","            except:\n","                continue\n","\n","        cv2.destroyAllWindows()\n","\n","        # Converting the list to numpy arrays\n","        # feature = np.asarray(feature)\n","        # origin_feature = np.asarray(origin_feature)\n","        # label = np.array(label)\n","        # np.savez_compressed(f'{DATASET_DIR}/{class_name}.npz', features=feature,\n","        #                     origin_features=origin_feature, labels=label, video_files_paths=video_files_path)\n","\n","    features = np.asarray(features)\n","    origin_features = np.asarray(origin_features)\n","    labels = np.array(labels)\n","    # Return the frames, class index, and video file path.\n","    return features, origin_features, labels, video_files_paths\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":374264,"status":"ok","timestamp":1679565093811,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"u-p5C3TLgOfH","outputId":"f20543f0-a922-47e6-ff4e-5aa5329d9560"},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracting Data of Class: Your\n","Extracting Data of Class: Love\n","Extracting Data of Class: Sleep\n","Extracting Data of Class: Name\n","Extracting Data of Class: What\n","Extracting Data of Class: Read\n","Extracting Data of Class: I\n","Extracting Data of Class: Bye\n","Extracting Data of Class: You\n","Extracting Data of Class: Eat\n","Extracting Data of Class: Hello\n"]}],"source":["features, origin_features, labels, video_files_paths = create_dataset()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1679565093815,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"pG_gfuEMgOfH"},"outputs":[],"source":["#import pickle\n","#with open('data_frame.pickle', 'w+') as f:\n","    #pickle.dump({'features': features.tolist(), 'labels': labels, 'video_files_paths': video_files_paths}, f, protocol=pickle.HIGHEST_PROTOCOL)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":107464,"status":"ok","timestamp":1679565201277,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"rWbGu3uKgOfI"},"outputs":[],"source":["np.savez_compressed(f'{DATASET_DIR}/data.npz', features=features, origin_features=origin_features,\n","                    labels=labels, video_files_paths=video_files_paths)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1679565201287,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"tzdZyxktgOfJ"},"outputs":[],"source":["dataset = np.load(f'{DATASET_DIR}/data.npz')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1679565201287,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"tXyfFpRWgOfK","outputId":"5fa851a4-ec7a-42e9-e9dd-4a35c6513e6b"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["video_reader = cv2.VideoCapture('/home/nhamcotdo/Downloads/train-20230322T100607Z-001/train/video750388.avi')\n","video_frames_count = 0\n","s, f = video_reader.read()\n","while s:\n","    s, f = video_reader.read()\n","    video_frames_count += 1\n","video_reader.release()\n","video_frames_count"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1679565201290,"user":{"displayName":"Nguyễn Phước Nhâm","userId":"01585936779914773185"},"user_tz":-420},"id":"xhOFeG9HgOfK","outputId":"2f34fe0e-97bb-4eaa-9a98-e2539b4380b5"},"outputs":[{"data":{"text/plain":["'./DaPhanLoai'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["DATASET_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeBem8Q-jyNy"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
